{"nbformat_minor": 2, "cells": [{"execution_count": 25, "cell_type": "code", "source": "import pandas as pd \nimport numpy as np \nimport os\n\n# Load the dataset\ndata = spark.read.csv(\"wasb://bigdatahadoophdistorage1-container@bigdatahadoophdistorage1.blob.core.windows.net/stockdata_small.csv\", header = True, mode = \"DROPMALFORMED\", inferSchema = True)\n\n# Convert into pandas Dataframe\ndf = data.toPandas()\n\n# Convert into wide format table then again from table to pandas dataframe\ndfwide = df.pivot(index='Date', columns = 'Ticker', values = 'PxChange')\ndf = pd.DataFrame(dfwide.to_records())\n\n# Drop the first Date column for Correlation\ndf_drop = df.drop(['Date'], axis=1)\n\n# Find the correlation between stocks\ndf_corr = df_drop.corr()\n\n# Convert the correlation dataframe into matrix\nmatrix = df_corr.as_matrix()\n\n# Find the number of rows and columns of the matrix\ncount_row = df_corr.shape[0]  \ncount_col = df_corr.shape[1]  \n\n# Create empty Adjacency Data Frame with 3 columns as TickerA, TickerB and Correlation\nadj_df = pd.DataFrame(columns=['TickerAA', 'dst', 'weight'])\n\n# Create a node dataframe for graph frame\nnode = list(df_corr.index)\n\n# Store the correlation values in the Adjacency Data Frame from correlation matrix\ni = 0\ncount = 0;\nfor i in range(count_row):\n    j = 0\n    for j in range(i):\n        if(j < i and j < count_col):\n            adj_df.loc[count,\"src\"] = node[i]\n            adj_df.loc[count,\"dst\"] = node[i-j-1]\n            adj_df.loc[count,\"weight\"] = matrix[i,j]\n            count = count + 1\n\nadj_df.shape   # see Adjacency Dataframe shape\n\n# Remove the column with NaN values\nadj_df_drop = adj_df.drop(['TickerAA'], axis=1)\n\n# Convert the correlation column to float\nadj_df_drop[\"weight\"]= adj_df_drop[\"weight\"].astype(float)\n\n# Two new columns to Adjacency Dataframe :- abs_weight and Color\nadj_df_drop['abs_weight'] = abs(adj_df_drop['weight'])\nadj_df_drop['Color'] = np.where(adj_df_drop['weight'] > 0, 'Red', 'Blue')\n\n# Rearange Adjancy Dataframe columns\nnew_order = [2,0,1,3,4]\nadjacency_df = adj_df_drop[adj_df_drop.columns[new_order]]\n\n# Create Vertices Dataframe\nnode = pd.DataFrame(node, columns = ['id'])\n\nadjacency_df.head(10)\n\n# Convert Link and Node dataframe into spark dataframe\ndf = spark.createDataFrame(adjacency_df)\ndf1 = spark.createDataFrame(node)\n\n# Save it into blob storage container\ndf.repartition(1)\\\n.write.format(\"csv\")\\\n.option(\"header\", True)\\\n.mode(\"overwrite\")\\\n.save(\"wasb://bigdatahadoophdistorage1-container@bigdatahadoophdistorage1.blob.core.windows.net/edge1.csv\",header = 'true')\n\ndf1.repartition(1)\\\n.write.format(\"csv\")\\\n.option(\"header\", True)\\\n.mode(\"overwrite\")\\\n.save(\"wasb://bigdatahadoophdistorage1-container@bigdatahadoophdistorage1.blob.core.windows.net/node1.csv\",header = 'true')", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 3281.634033203125, "end_time": 1574253895618.7}}, "collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}