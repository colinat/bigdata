{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 953.22607421875,
      "end_time": 1574467847642.469
     }
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1574406713992_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-bigdat.5basapwjp0aedchpxvracyfp0f.bx.internal.cloudapp.net:8088/proxy/application_1574406713992_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn4-bigdat.5basapwjp0aedchpxvracyfp0f.bx.internal.cloudapp.net:30060/node/containerlogs/container_e03_1574406713992_0002_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "/usr/bin/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 9624.549072265625,
      "end_time": 1574467865540.073
     }
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "#data = spark.read.csv(\"wasb://bigdatahadoophdistorage1-container@bigdatahadoophdistorage1.blob.core.windows.net/stockdata_small.csv\", header = True, mode = \"DROPMALFORMED\", inferSchema = True)\n",
    "data = spark.read.csv(\"wasb://datastore1@bigdataclustehdistorage1.blob.core.windows.net/stockdata_small.csv\", header = True, mode = \"DROPMALFORMED\", inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3793.83203125,
      "end_time": 1574467886495.67
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert into pandas Dataframe\n",
    "df = data.toPandas()\n",
    "\n",
    "# Create Vertices Dataframe\n",
    "date = df['Date'].max()\n",
    "df2 = df[df['Date'] == date]\n",
    "node = df2.loc[:,['Ticker','Name','Price']]\n",
    "node.columns = ['id','id_name','id_price']\n",
    "node.head(10)\n",
    "\n",
    "# Convert into wide format table then again from table to pandas dataframe\n",
    "dfwide = df.pivot(index='Date', columns = 'Ticker', values = 'PxChange')\n",
    "df = pd.DataFrame(dfwide.to_records())\n",
    "\n",
    "# Drop the first Date column for Correlation\n",
    "df_drop = df.drop(['Date'], axis=1)\n",
    "\n",
    "# Find the correlation between stocks\n",
    "df_corr = df_drop.corr()\n",
    "\n",
    "# Convert the correlation dataframe into matrix\n",
    "matrix = df_corr.as_matrix()\n",
    "\n",
    "# Find the number of rows and columns of the matrix\n",
    "count_row = df_corr.shape[0]  \n",
    "count_col = df_corr.shape[1]  \n",
    "\n",
    "# Create empty Adjacency Data Frame with 3 columns as TickerA, TickerB and Correlation\n",
    "adj_df = pd.DataFrame(columns=['TickerAA', 'dst', 'weight'])\n",
    "\n",
    "# Create a node dataframe for graph frame\n",
    "ticker = list(df_corr.index)\n",
    "\n",
    "# Store the correlation values in the Adjacency Data Frame from correlation matrix\n",
    "i = 0\n",
    "count = 0;\n",
    "for i in range(count_row):\n",
    "    j = 0\n",
    "    for j in range(i):\n",
    "        if(j < i and j < count_col):\n",
    "            adj_df.loc[count,\"src\"] = ticker[i]\n",
    "            adj_df.loc[count,\"dst\"] = ticker[i-j-1]\n",
    "            adj_df.loc[count,\"weight\"] = matrix[i,j]\n",
    "            count = count + 1\n",
    "\n",
    "adj_df.shape   # see Adjacency Dataframe shape\n",
    "\n",
    "# Remove the column with NaN values\n",
    "adj_df_drop = adj_df.drop(['TickerAA'], axis=1)\n",
    "\n",
    "# Convert the correlation column to float\n",
    "adj_df_drop[\"weight\"]= adj_df_drop[\"weight\"].astype(float)\n",
    "\n",
    "# Two new columns to Adjacency Dataframe :- abs_weight and Color\n",
    "adj_df_drop['abs_weight'] = abs(adj_df_drop['weight'])\n",
    "adj_df_drop['Color'] = np.where(adj_df_drop['weight'] > 0, 'Blue', 'Red')\n",
    "\n",
    "# Threshold value for correlation is 0.6\n",
    "#adj_df_drop['abs_weight'] = np.where(adj_df_drop['abs_weight'] < 0.6, 0,adj_df_drop['abs_weight'])\n",
    "#temp = adj_df_drop[adj_df_drop['abs_weight'] >= 0.6]\n",
    "\n",
    "# Rearange Adjancy Dataframe columns\n",
    "new_order = [2,0,1,3,4]\n",
    "#adjacency_df = temp[temp.columns[new_order]]\n",
    "adjacency_df = adj_df_drop[adj_df_drop.columns[new_order]]\n",
    "\n",
    "adjacency_df.head(10)\n",
    "\n",
    "# Convert Link and Node dataframe into spark dataframe\n",
    "edge = spark.createDataFrame(adjacency_df)\n",
    "node = spark.createDataFrame(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 5564.56396484375,
      "end_time": 1574467909633.999
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save it into blob storage container\n",
    "edge.repartition(1)\\\n",
    ".write.format(\"csv\")\\\n",
    ".option(\"header\", True)\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(\"wasb://datastore1@bigdataclustehdistorage1.blob.core.windows.net/edge\",header = 'true')\n",
    "#.save(\"wasb://bigdatahadoophdistorage1-container@bigdatahadoophdistorage1.blob.core.windows.net/edge\",header = 'true')\n",
    "\n",
    "node.repartition(1)\\\n",
    ".write.format(\"csv\")\\\n",
    ".option(\"header\", True)\\\n",
    ".mode(\"overwrite\")\\\n",
    ".save(\"wasb://datastore1@bigdataclustehdistorage1.blob.core.windows.net/node\",header = 'true')\n",
    "#.save(\"wasb://bigdatahadoophdistorage1-container@bigdatahadoophdistorage1.blob.core.windows.net/node\",header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}