{"nbformat_minor": 2, "cells": [{"execution_count": 2, "cell_type": "code", "source": "import pandas as pd \nimport numpy as np \n\n# Load the dataset\n#data = spark.read.csv(\"wasb://bigdatahadoophdistorage1-container@bigdatahadoophdistorage1.blob.core.windows.net/stockdata_small.csv\", header = True, mode = \"DROPMALFORMED\", inferSchema = True)\ndata = spark.read.csv(\"wasb://datastore1@bigdata1hdistorage.blob.core.windows.net/stockdata_small.csv\", header = True, mode = \"DROPMALFORMED\", inferSchema = True)\n\n# Convert into pandas Dataframe\ndf = data.toPandas()\n\n# Create Vertices Dataframe\ndate = df['Date'].max()\ndf2 = df[df['Date'] == date]\nnode = df2.loc[:,['Ticker','Name','Price']]\nnode.columns = ['id','id_name','id_price']\nnode.head(10)\n\n# Convert into wide format table then again from table to pandas dataframe\ndfwide = df.pivot(index='Date', columns = 'Ticker', values = 'PxChange')\ndf = pd.DataFrame(dfwide.to_records())\n\n# Drop the first Date column for Correlation\ndf_drop = df.drop(['Date'], axis=1)\n\n# Find the correlation between stocks\ndf_corr = df_drop.corr()\n\n# Convert the correlation dataframe into matrix\nmatrix = df_corr.as_matrix()\n\n# Find the number of rows and columns of the matrix\ncount_row = df_corr.shape[0]  \ncount_col = df_corr.shape[1]  \n\n# Create empty Adjacency Data Frame with 3 columns as TickerA, TickerB and Correlation\nadj_df = pd.DataFrame(columns=['TickerAA', 'dst', 'weight'])\n\n# Create a node dataframe for graph frame\nticker = list(df_corr.index)\n\n# Store the correlation values in the Adjacency Data Frame from correlation matrix\ni = 0\ncount = 0;\nfor i in range(count_row):\n    j = 0\n    for j in range(i):\n        if(j < i and j < count_col):\n            adj_df.loc[count,\"src\"] = ticker[i]\n            adj_df.loc[count,\"dst\"] = ticker[i-j-1]\n            adj_df.loc[count,\"weight\"] = matrix[i,j]\n            count = count + 1\n\nadj_df.shape   # see Adjacency Dataframe shape\n\n# Remove the column with NaN values\nadj_df_drop = adj_df.drop(['TickerAA'], axis=1)\n\n# Convert the correlation column to float\nadj_df_drop[\"weight\"]= adj_df_drop[\"weight\"].astype(float)\n\n# Two new columns to Adjacency Dataframe :- abs_weight and Color\nadj_df_drop['abs_weight'] = abs(adj_df_drop['weight'])\nadj_df_drop['Color'] = np.where(adj_df_drop['weight'] > 0, 'Blue', 'Red')\n\n# Threshold value for correlation is 0.6\n#adj_df_drop['abs_weight'] = np.where(adj_df_drop['abs_weight'] < 0.6, 0,adj_df_drop['abs_weight'])\n#temp = adj_df_drop[adj_df_drop['abs_weight'] >= 0.6]\n\n# Rearange Adjancy Dataframe columns\nnew_order = [2,0,1,3,4]\n#adjacency_df = temp[temp.columns[new_order]]\nadjacency_df = adj_df_drop[adj_df_drop.columns[new_order]]\n\nadjacency_df.head(10)\n\n# Convert Link and Node dataframe into spark dataframe\nedge = spark.createDataFrame(adjacency_df)\nnode = spark.createDataFrame(node)\n\n# Save it into blob storage container\nedge.repartition(1)\\\n.write.format(\"csv\")\\\n.option(\"header\", True)\\\n.mode(\"overwrite\")\\\n.save(\"wasb://datastore1@bigdata1hdistorage.blob.core.windows.net/edge\",header = 'true')\n#.save(\"wasb://bigdatahadoophdistorage1-container@bigdatahadoophdistorage1.blob.core.windows.net/edge\",header = 'true')\n\nnode.repartition(1)\\\n.write.format(\"csv\")\\\n.option(\"header\", True)\\\n.mode(\"overwrite\")\\\n.save(\"wasb://datastore1@bigdata1hdistorage.blob.core.windows.net/node\",header = 'true')\n#.save(\"wasb://bigdatahadoophdistorage1-container@bigdatahadoophdistorage1.blob.core.windows.net/node\",header = 'true')", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 7303.68896484375, "end_time": 1574312307055.088}}, "collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}